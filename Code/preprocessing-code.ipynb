{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q emoji\n!pip install -q vncorenlp\n!pip install -q datasets\n!pip install -q transformers\n!pip install -q py_vncorenlp","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:31:59.633286Z","iopub.execute_input":"2024-06-27T03:31:59.633688Z","iopub.status.idle":"2024-06-27T03:33:41.483206Z","shell.execute_reply.started":"2024-06-27T03:31:59.633652Z","shell.execute_reply":"2024-06-27T03:33:41.481407Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport emoji\nimport vncorenlp\nimport datasets\nimport transformers\nimport py_vncorenlp\nimport regex as re\nfrom py_vncorenlp import VnCoreNLP","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:38:30.190938Z","iopub.execute_input":"2024-06-27T03:38:30.191410Z","iopub.status.idle":"2024-06-27T03:38:30.197889Z","shell.execute_reply.started":"2024-06-27T03:38:30.191375Z","shell.execute_reply":"2024-06-27T03:38:30.196547Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install openjdk-11-jdk-headless -qq > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:39:09.917831Z","iopub.execute_input":"2024-06-27T03:39:09.918815Z","iopub.status.idle":"2024-06-27T03:39:40.909075Z","shell.execute_reply.started":"2024-06-27T03:39:09.918769Z","shell.execute_reply":"2024-06-27T03:39:40.907445Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]    \nGet:3 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1225 B]\nGet:4 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]       \nHit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \nGet:6 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\nGet:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3071 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1470 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1512 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3867 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4218 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1227 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3746 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3712 kB]\nFetched 23.1 MB in 2s (11.4 MB/s)                           \nReading package lists... Done\n","output_type":"stream"}]},{"cell_type":"code","source":"py_vncorenlp.download_model(save_dir='/kaggle/working')\nannotator = VnCoreNLP(save_dir='/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:39:40.912397Z","iopub.execute_input":"2024-06-27T03:39:40.912949Z","iopub.status.idle":"2024-06-27T03:40:08.234822Z","shell.execute_reply.started":"2024-06-27T03:39:40.912890Z","shell.execute_reply":"2024-06-27T03:40:08.232986Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"VnCoreNLP model folder /kaggle/working already exists! Please load VnCoreNLP from this folder!\n2024-06-27 03:39:41 INFO  WordSegmenter:24 - Loading Word Segmentation model\n2024-06-27 03:39:41 INFO  PosTagger:23 - Loading POS Tagging model\n2024-06-27 03:39:45 INFO  NerRecognizer:34 - Loading NER model\n2024-06-27 03:40:04 INFO  DependencyParser:32 - Loading Dependency Parsing model\n","output_type":"stream"}]},{"cell_type":"code","source":"VN_CHARS = 'áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ'\n\nVOWELS_TABLE = [\n    ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']\n]\n\n\nVOWELS_TO_IDS = {\n    'a': (0, 0), 'à': (0, 1), 'á': (0, 2), 'ả': (0, 3), 'ã': (0, 4), 'ạ': (0, 5), \n    'ă': (1, 0), 'ằ': (1, 1), 'ắ': (1, 2), 'ẳ': (1, 3), 'ẵ': (1, 4), 'ặ': (1, 5), \n    'â': (2, 0), 'ầ': (2, 1), 'ấ': (2, 2), 'ẩ': (2, 3), 'ẫ': (2, 4), 'ậ': (2, 5), \n    'e': (3, 0), 'è': (3, 1), 'é': (3, 2), 'ẻ': (3, 3), 'ẽ': (3, 4), 'ẹ': (3, 5), \n    'ê': (4, 0), 'ề': (4, 1), 'ế': (4, 2), 'ể': (4, 3), 'ễ': (4, 4), 'ệ': (4, 5), \n    'i': (5, 0), 'ì': (5, 1), 'í': (5, 2), 'ỉ': (5, 3), 'ĩ': (5, 4), 'ị': (5, 5), \n    'o': (6, 0), 'ò': (6, 1), 'ó': (6, 2), 'ỏ': (6, 3), 'õ': (6, 4), 'ọ': (6, 5), \n    'ô': (7, 0), 'ồ': (7, 1), 'ố': (7, 2), 'ổ': (7, 3), 'ỗ': (7, 4), 'ộ': (7, 5), \n    'ơ': (8, 0), 'ờ': (8, 1), 'ớ': (8, 2), 'ở': (8, 3), 'ỡ': (8, 4), 'ợ': (8, 5), \n    'u': (9, 0), 'ù': (9, 1), 'ú': (9, 2), 'ủ': (9, 3), 'ũ': (9, 4), 'ụ': (9, 5), \n    'ư': (10, 0), 'ừ': (10, 1), 'ứ': (10, 2), 'ử': (10, 3), 'ữ': (10, 4), 'ự': (10, 5), \n    'y': (11, 0), 'ỳ': (11, 1), 'ý': (11, 2), 'ỷ': (11, 3), 'ỹ': (11, 4), 'ỵ': (11, 5)\n}\n\nVINAI_NORMALIZED_TONE = {\n    'òa': 'oà', 'Òa': 'Oà', 'ÒA': 'OÀ', \n    'óa': 'oá', 'Óa': 'Oá', 'ÓA': 'OÁ', \n    'ỏa': 'oả', 'Ỏa': 'Oả', 'ỎA': 'OẢ',\n    'õa': 'oã', 'Õa': 'Oã', 'ÕA': 'OÃ',\n    'ọa': 'oạ', 'Ọa': 'Oạ', 'ỌA': 'OẠ',\n    'òe': 'oè', 'Òe': 'Oè', 'ÒE': 'OÈ',\n    'óe': 'oé', 'Óe': 'Oé', 'ÓE': 'OÉ',\n    'ỏe': 'oẻ', 'Ỏe': 'Oẻ', 'ỎE': 'OẺ',\n    'õe': 'oẽ', 'Õe': 'Oẽ', 'ÕE': 'OẼ',\n    'ọe': 'oẹ', 'Ọe': 'Oẹ', 'ỌE': 'OẸ',\n    'ùy': 'uỳ', 'Ùy': 'Uỳ', 'ÙY': 'UỲ',\n    'úy': 'uý', 'Úy': 'Uý', 'ÚY': 'UÝ',\n    'ủy': 'uỷ', 'Ủy': 'Uỷ', 'ỦY': 'UỶ',\n    'ũy': 'uỹ', 'Ũy': 'Uỹ', 'ŨY': 'UỸ',\n    'ụy': 'uỵ', 'Ụy': 'Uỵ', 'ỤY': 'UỴ',\n}\n\nwith open('kltn/teencode.txt', 'r', encoding = 'utf8') as f:\n    data = f.readlines()\n    data = [re.sub(r'[,‘’\\';]', '', i).lower() for i in data]\n    data = [i.replace('\\n', '').replace('\\ufeff', '') for i in data]\n    slangs_dict = {}\n    for i in data:\n        j = i.split(':')\n        slangs_dict[j[0].strip()] = j[1].strip()\n\nwith open('kltn/vietnamese_words.txt', 'r', encoding = 'utf8') as f:\n    data = f.readlines()\n    vietnamese_words_set = set()\n    for i in data:\n        vietnamese_words_set.add(i.strip())","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:35:36.364570Z","iopub.execute_input":"2024-06-27T03:35:36.365163Z","iopub.status.idle":"2024-06-27T03:35:36.428847Z","shell.execute_reply.started":"2024-06-27T03:35:36.365116Z","shell.execute_reply":"2024-06-27T03:35:36.427685Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def clean_space(text):\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\r', ' ')\n    return text\n\n\ndef remove_html(text):\n    return re.sub(r'<[^>]*>', ' ', text)\n\n\ndef remove_emoji(text):\n    return emoji.replace_emoji(text, ' ')\n\n\ndef remove_url(text):\n    return re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)', ' ', text)\n\n\ndef remove_email(text):\n    return re.sub(r'[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+', ' ', text)\n\n\ndef remove_phone_number(text):\n    return re.sub(r'^[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}$', ' ', text)\n\n\ndef remove_unnecessary_characters(text):\n    return re.sub(fr\"[^\\s\\w{VN_CHARS}]\", ' ', text)\n\n\ndef normalize_unicode(text):\n    char1252 = r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n    charutf8 = r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n    char_map = dict(zip(char1252.split('|'), charutf8.split('|')))\n    return re.sub(char1252, lambda x: char_map[x.group()], text.strip())\n\n\ndef normalize_sentence_typing(text, vinai_normalization=False):\n    # https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md\n    if vinai_normalization: # Just simply replace the wrong tone with the correct one defined by VinAI\n        for wrong, correct in VINAI_NORMALIZED_TONE.items():\n            text = text.replace(wrong, correct)\n        return text.strip()\n\n    # Or you can use this algorithm developed by Behitek to normalize Vietnamese typing in a sentence \n    words = text.strip().split()\n    for index, word in enumerate(words):\n        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n        if len(cw) == 3: cw[1] = normalize_word_typing(cw[1])\n        words[index] = ''.join(cw)\n    return ' '.join(words)\n\n\ndef normalize_word_typing(word):\n    if not is_valid_vietnamese_word(word): return word\n    chars, vowel_indexes = list(word), []\n    qu_or_gi, tonal_mark = False, 0\n\n    for index, char in enumerate(chars):\n        if char not in VOWELS_TO_IDS: continue\n        row, col = VOWELS_TO_IDS[char]\n        if index > 0 and (row, chars[index - 1]) in [(9, 'q'), (5, 'g')]:\n            chars[index] = VOWELS_TABLE[row][0]\n            qu_or_gi = True\n\n        if not qu_or_gi or index != 1: vowel_indexes.append(index)\n        if col != 0:\n            tonal_mark = col\n            chars[index] = VOWELS_TABLE[row][0]\n\n    if len(vowel_indexes) < 2:\n        if qu_or_gi:\n            index = 1 if len(chars) == 2 else 2\n            if chars[index] in VOWELS_TO_IDS:\n                row, _ = VOWELS_TO_IDS[chars[index]]\n                chars[index] = VOWELS_TABLE[row][tonal_mark]\n            else: chars[1] = VOWELS_TABLE[5 if chars[1] == 'i' else 9][tonal_mark]\n            return ''.join(chars)\n        return word\n\n    for index in vowel_indexes:\n        row, _ = VOWELS_TO_IDS[chars[index]]\n        if row in [4, 8]:\n            chars[index] = VOWELS_TABLE[row][tonal_mark]\n            return ''.join(chars)\n\n    index = vowel_indexes[0 if len(vowel_indexes) == 2 and vowel_indexes[-1] == len(chars) - 1 else 1] \n    row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]\n    chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]\n    return ''.join(chars)\n\n\ndef is_valid_vietnamese_word(word):\n    vowel_indexes = -1 \n    for index, char in enumerate(word):\n        if char not in VOWELS_TO_IDS: continue\n        if vowel_indexes in [-1, index - 1]: vowel_indexes = index\n        else: return False\n    return True\n\n\ndef fix_slangs(text):\n    words = text.split()\n    fixed_words = [slangs_dict.get(word.lower(), word) for word in words]\n    text = ' '.join(fixed_words)\n    return text\n\n\ndef word_processing(text):\n    \n    # Tách các từ trong chuỗi\n    words = text.split()\n    \n    processed_words = []\n\n    for word in words:\n        \n        if word.isdigit():\n            processed_words.append(word)\n            continue\n        \n        if word.lower() not in vietnamese_words_set:\n            # Xác định chữ cái cuối cùng\n            last_char = word[-1]\n\n            # Tìm vị trí của ký tự cuối cùng bị lặp\n            index = None\n            for i in range(len(word)-1, -1, -1):\n                if word[i] != last_char:\n                    index = i + 1\n                    break\n\n            # Loại bỏ các ký tự cuối cùng dư thừa\n            if index is not None:\n                word = word[:index+1]\n            \n        # Loại bỏ các từ vô nghĩa\n        if word.lower() not in vietnamese_words_set:\n            continue\n\n        processed_words.append(word)\n\n    # Tạo chuỗi kết quả\n    result = ' '.join(processed_words)   \n    return result\n\n\n# Word segmentation\ndef word_segmentation(text):\n    segmented_text = annotator.word_segment(text)\n    return ' '.join(segmented_text)\n\n\ndef text_preprocess(text):\n    text = clean_space(text)\n    text = remove_html(text)\n    text = remove_emoji(text)\n    text = remove_url(text)\n    text = remove_email(text)\n    text = remove_phone_number(text)\n    text = remove_unnecessary_characters(text)\n    text = normalize_unicode(text)\n    text = normalize_sentence_typing(text, True)\n    text = fix_slangs(text)\n    text = word_processing(text)\n    text = word_segmentation(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-27T03:35:38.939380Z","iopub.execute_input":"2024-06-27T03:35:38.939913Z","iopub.status.idle":"2024-06-27T03:35:38.976987Z","shell.execute_reply.started":"2024-06-27T03:35:38.939876Z","shell.execute_reply":"2024-06-27T03:35:38.975495Z"},"trusted":true},"execution_count":11,"outputs":[]}]}